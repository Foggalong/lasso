---
title: "Sparse Regression Models"
subtitle: "Lecture 6 - MATH11301"
format:
  revealjs:
    lib_dir: libs
    smaller: true
    theme: [default] #, edtheme.scss]
---

## Outline

```{r}
#| label: r-setup
#| echo: false
#| warning: false
#| message: false
options(htmltools.dir.version = FALSE)
library(tidyverse)   # kitchen sink
library(kableExtra)  # table formatting
library(ggpubr)      # annotations like stat_regline_equation
library(openintro)   # credit dataset from IDS
library(broom)       # keep it tidy()
library(glmnet)      # Ridge & Lasso implementations
library(Matrix)      # glmnet dependency, input type
```

1. Regression Recap
2. Subset Selection
3. Shrinkage Methods
4. Dimension Reduction

```{r}
#| label: opening-xkcd
#| fig-align: "right"
#| fig-alt: "Two square panels show identical sets of scattered black dots, with only the red additions being different. The left panel shows a slightly rising red line drawn through the middle of the panel, passing near a few dots but not obviously related to most of them. A red text is below the dots: R-squared = 0.06. The right panel shows many of the dots connected by red lines to form a stick figure of a man resembling the constellation Orion, with the hand on the reader's right raised and holding an object. A red text is below the dots: Rexthor, the Dog-Bearer. A caption is below and spanning both panels: I don't trust linear regressions when it's harder to guess the direction of the correlation from the scatter plot than to find new constellations on it."
#| out-width: "50%"
knitr::include_graphics("https://imgs.xkcd.com/comics/linear_regression.png")
```

# 1. Regression Recap

## Simple Linear Regression

Given response $y$ and **single** predictor $x$, this model fits to $y = \beta_{0} + \beta_{1}x$.

```{r}
#| label: scatter-plot
#| echo: false
#| out-width: '80%'
#| fig-align: 'center'
#| fig-format: svg
#| fig-alt: "Scatter diagram for the classic 'mtcars' dataset, showing miles per gallon against horsepower. A linear regression of y = 30 - 0.068x is shown."
scatter_example <- ggplot(mtcars, aes(x = hp, y = mpg)) +
  theme_minimal() +
  labs(
    title = "Scatter Plot of MPG against Horsepower",
    subtitle = "Looking at 32 cars from 'Motor Trend' magazine (1974).",
    x = "horsepower, x",
    y = "miles per gallon, y",
  ) +
  ylim(0, 35)

# for first example, all point the same
scatter_example +
  geom_point(size=4, alpha=0.33, color="blue") +
  geom_smooth(formula=y~x, method="lm", se=FALSE) +
  stat_regline_equation(label.x=250, label.y=30)
```

For $n$ observations $(x^{(i)}, y^{(i)})$, the fit is found by minimizing the residual sum of squares:

\begin{equation}
  \min_{\beta_0,\beta_1} \left( \sum_{i=1}^n {\left({ y^{(i)} - \beta_0 - \beta_{1}x^{(i)} }\right)}^2 \right).
\end{equation}

## Multiple Linear Regression

Typically in real data, we have **multiple** predictors for a response variable.

Given $n$ observations of

- response variable $y$,
- vector of $p$ predictors, ${\bf x} = (x_{1}, x_{2}, \ldots, x_{p})$,

the linear regression model is

$$
 y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_px_p + \varepsilon,
$$

again, fit using least squares procedure:

\begin{equation}
  \min_{\beta_0, \beta_1, \ldots, \beta_p} \left(\sum_{i=1}^n {\left( y^{(i)} - \beta_0 - \sum_{j=1}^p\beta_{j}x_{j}^{(i)} \right)}^2 \right) =: \min_{{\bf\beta}}(\operatorname{RSS}).
\end{equation}

## What are the practical issues here?
<!-- class: inverse, middle, center -->

$$
 y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_px_p
$$

## Example Linear Model

In <abbr title="Introduction to Data Science, one of the first year maths courses here in Edinburgh.">IDS</abbr>, we fit a linear model to the [Lending Club's `loans` dataset](https://openintrostat.github.io/openintro/reference/loans_full_schema.html).

```{r}
#| label: ids-data-prep
#| echo: false
# IDS uses 'Introduction to Modern Statistics' textbook. It doesn't actually
# work with the full, raw Lending Club data; using the code below it first
# consolidates some variables, fixes some types, and then selects a subset.

loans_ids <- loans_full_schema |>
  mutate(
    credit_util = total_credit_utilized / total_credit_limit,
    bankruptcy = as.factor(if_else(public_record_bankrupt == 0, 0, 1)),
    income_ = droplevels(verified_income)
  ) |>
  rename(
    credit_checks = inquiries_last_12m
  ) |>
  select(
    interest_rate,
    income_,
    debt_to_income,
    credit_util,
    bankruptcy,
    term,
    credit_checks,
    issue_month
  )
```

```{r}
#| label: ims-analyse
#| echo: false
# This code for MLR is also from Intro to Modern Stats, just with a few
# minor formatting corrections.

model_ids <- lm(interest_rate~., data=loans_ids)

model_ids |>
  tidy() |>
  mutate(p.value = ifelse(p.value < 0.001, "< 0.001", round(p.value, 3))) |>
  kbl(linesep="", booktabs=TRUE, digits=2, align="lrrrr") |>
  kable_styling(
    bootstrap_options = c("striped", "condensed"),
    latex_options = c("striped")
  ) |>
  column_spec(1, width="15em", monospace=TRUE) |>
  column_spec(2:5, width="5em")
```

# 2. Subset Selection
<!-- class: inverse, middle, center -->

## Model Selection

We have a model that works with all the predictors, but could we do better?

> _"Of two competing theories, the simpler explanation is to be preferred."_ (Ockham's Razor)

. . .

Recall, to compare models of differing size we can use a metric such as the **Akaike information criterion**, which for least squares is

\begin{equation}
  \operatorname{AIC} = \frac{1}{n}\left( {\operatorname{RSS} + 2d\hat{\sigma}^2} \right).
\end{equation}

or the adjusted $R^2$ value,

\begin{equation}
  \operatorname{Adjusted}~R^2 = 1 - \frac{\operatorname{RSS}/(n-d-1)}{\operatorname{TSS}/(n-1)},
\end{equation}

where $\operatorname{TSS} = \sum_i {(y_i-\bar{y})}^2$ and $d$ is the number of variables in the model.

## Backward Selection

Starting with full model, successively remove predictors until the criterion can't improve.

```{r}
#| label: step-backwards
#| echo: true
full_model <- lm(interest_rate~., data=loans_ids)
backward <- step(full_model, direction='backward')
```

## Forward Selection

Starting with the empty model, successively add predictors until criterion can't improve.

```{r}
#| label: step-forwards
#| echo: true
empty_model <- lm(interest_rate~1, data=na.omit(loans_ids))
forward <- step(empty_model, direction='forward', scope = formula(full_model))
```

## Hybrid Approaches

Backward selection requires that $n > p$ in order to yield a unique least-squares solution. Forward selection works with high-dimensional data when $n < p$ (potentially $n \ll p$).

. . .

<!-- Both beat exhaustion, going from checking $2^p$ models ([NP-complete](https://en.wikipedia.org/wiki/NP-completeness)) to $1+\frac{p(p+1)}{2}$ ([P](https://en.wikipedia.org/wiki/P_(complexity)), but neither are guaranteed to find the optimal $d$-predictor model. -->

In practice, we often take combined approach: starting from the empty model, whenever a new variable is added, remove any now-obsolete variables.

```{r}
#| label: step-both
#| echo: true
#| eval: false
empty_model <- lm(interest_rate~1, data=na.omit(loans_ids))
both <- step(empty_model, direction='both', scope = formula(full_model))
```

The aim is to mimic the 'benefits' of an exhaustive search while retaining subset selection's efficiency.
. . .
 Importantly though, **they do not guarantee optimality!**

. . .

Both work well if $p \ll d$, but **neither is performant when we expect the model to use even a moderate number of variables.**

# 3. Shrinkage Methods
<!-- class: inverse, middle, center -->

## Penalty Terms

Subset selection isn't the only way improve models. Here's a more _optimum_ approach.

. . .

To penalise models whose coefficient vectors have become too 'unruly', we use a penalty

\begin{equation}
  P(\beta) = \lambda \|\beta\|,
\end{equation}

where $\|\beta\|$ is a vector-norm and $\lambda\geq0$ is a tuning parameter. 

. . .

Working from the least squares problem, $\min_{{\bf\beta}}(\operatorname{RSS})$, we add this to the objective to give

\begin{equation}
  \min_{\beta} \left(\sum_{i=1}^n {\left( y^{(i)} - \beta_0 - \sum_{j=1}^p\beta_{j}x_{j}^{(i)} \right)}^2 + \lambda\|\beta\| \right),
\end{equation}

::: aside
**Note**: we can show the above is the **Lagrange form** of constraining ;east squares by some ${\|\beta\|} \leq t$.
:::

## The Ridge

In **Ridge regression**, we apply the $\mathcal{l}_2$ norm, solving $\min_{\beta} (RSS + \lambda\sum_{i=1}^p\beta_j^2 )$.

```{r}
#| label: load-data-nona
#| echo: false
loans_ids <- na.omit(loans_ids)
```

```{r}
#| label: ridge-regression
#| warning: false
#| message: false
# glmnet requires matrix input, not formula
X <- as.matrix(loans_ids[, -1])  # first column is interest_rate
y <- as.matrix(loans_ids[, 1])
model = glmnet(X, y, alpha = 0)  # use L2 norm
```

```{r}
#| label: ridge-coefficient-plot
#| echo: false
#| fig-align: "center"
#| fig-height: 4
#| fig.width: 6
#| fig-format: svg
plot(model, label = TRUE, xvar = "lambda")
```

Note that $\beta_i\neq0$ for most $i$ and $\lambda$ here.

## The Lasso

With **Lasso regression** we apply the $\mathcal{l}_1$ norm, solving $\min_{\beta} (RSS + \lambda\sum_{i=1}^p|\beta_j^{\phantom{'}}| )$.

```{r}
#| label: lasso-regression
#| warning: false
#| message: false
# glmnet requires matrix input, not formula
X <- as.matrix(loans_ids[, -1])  # first column is interest_rate
y <- as.matrix(loans_ids[, 1])
model = glmnet(X, y, alpha = 1)  # use L1 norm
```

```{r}
#| label: lasso-coefficient-plot
#| echo: false
#| fig-align: "center"
#| fig-height: 4
#| fig-width: 6
#| fig-format: svg
plot(model, label = TRUE, xvar = "lambda")
```

Note that $\beta_i=0$ for some $i$ and $\lambda$ here.

## What's the key difference here?
<!-- class: inverse, middle, center -->

$$
  l_1~\text{vs.}~l_2
$$

## Constraint Comparison

```{r}
#| label: figure-6.7
#| echo: false
#| out-width: "80%"
#| fig-show: "hold"
#| fig-align: "center"
#| fig-alt: "Hello, world!"
#| fig-cap: "Contour plot of the error and constraint functions for the lasso and ridge respectively."
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/5/58/Regularization.jpg")
```

## Larger Dataset

Unlike Ridge, Lasso performs variable selection. This creates **sparse models**.

```{r}
#| label: tall-data-simulation
#| results: 'hide'
n <- 100
X <- matrix(rnorm(n*20), n, 20)  # n-by-20 matrix of rnorms
y <- rnorm(n)
ridge_model <- glmnet(X, y, alpha=0)  # L2 norm
lasso_model <- glmnet(X, y, alpha=1)  # L1 norm
```

```{r}
#| label: tall-data-analysis
#| echo: false
#| fig-format: svg
#| layout-ncol: 2
#| fig-cap:
#|   - "Ridge Coefficients"
#|   - "Lasso Coefficients"
#| fig-cap-location: top  # BUG doesn't seem to work
plot(ridge_model, xvar="lambda")
plot(lasso_model, xvar="lambda")
```

## Elastic Net

Could instead use a combined penalty term, $P(\alpha,\beta) = \frac{1}{2}(1-\alpha){\|\beta\|}_2^2 + \alpha{\|\beta\|}_1$. 

```{r}
#| label: elastic-net
#| echo: false
#| fig-format: svg
#| layout-ncol: 3
#| layout-nrow: 2
#| fig-cap:
#|   - "alpha = 0"
#|   - "alpha = 0.001"
#|   - "alpha = 0.01"
#|   - "alpha = 0.1"
#|   - "alpha = 0.5"
#|   - "alpha = 1"
#| fig-cap-location: top  # BUG doesn't seem to work
for (a in c(0, 1e-3, 1e-2, 1e-1, 5e-1, 1)) {
  plot(glmnet(X, y, alpha=a), xvar="lambda")
}
```

# 4. Dimension Reduction
<!-- class: inverse, middle, center -->


## Placeholder

```{r}
#| label: dimension-reduction-placeholder
summary(mtcars)  # filler for a slide we should never get to
```

# End
<!-- class: inverse, middle, center -->
