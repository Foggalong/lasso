---
title: "Sparse Modeling & Lasso"
subtitle: "Lecture 6 - MATH11301" 
author: "Josh Fogg"

output:
  xaringan::moon_reader:
    lib_dir: libs
    css: ["default", "edtheme.scss"]
    nature:
      # highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

<!-- Blocks that do setup for the rest of the document-->
```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(kableExtra)
library(ggpubr)
library(openintro)
library(broom)
library(tippy)
```

```{css, echo=FALSE}
.scroll-300 {
  /* enable scrolling output for large code blocks */
  max-height: 300px;
  overflow-y: auto;
}
```

## Outline

<font size="6"> <!--Create diagonal with contents and comic-->
  1. Regression Recap<br>
  2. Subset Selection<br>
  3. Shrinkage Methods<br>
  4. Dimension Reduction<br>
</font>

```{r opening-xkcd, echo=FALSE, out.width="50%", fig.show="hold",  fig.align="right", fig.alt="Two square panels show identical sets of scattered black dots, with only the red additions being different. The left panel shows a slightly rising red line drawn through the middle of the panel, passing near a few dots but not obviously related to most of them. A red text is below the dots: R-squared = 0.06. The right panel shows many of the dots connected by red lines to form a stick figure of a man resembling the constellation Orion, with the hand on the reader's right raised and holding an object. A red text is below the dots: Rexthor, the Dog-Bearer. A caption is below and spanning both panels: I don't trust linear regressions when it's harder to guess the direction of the correlation from the scatter plot than to find new constellations on it."}
knitr::include_graphics("https://imgs.xkcd.com/comics/linear_regression.png")
```

---
class: inverse, middle, center

# 1. Regression Recap

---

## Simple Linear Regression

Given response $y$ and **single** predictor $x$, this model fits to $y = \beta_{0} + \beta_{1}x$.

```{r scatter-plot, echo=FALSE, fig.align='center', fig.height=4, fig.width=8, dev='svg'}
# plot features that will be common for both
scatter_example <- ggplot(mtcars, aes(x = hp, y = mpg)) +
  theme_minimal() +
  labs(
    title = "Scatter Plot of MPG against Horsepower",
    subtitle = "Looking at 32 cars from 'Motor Trend' magazine (1974).",
    x = "horsepower, x",
    y = "miles per gallon, y",
  ) +
  ylim(0,35)

# for first example, all point the same
scatter_example +
  geom_point(size=4, alpha=0.33, color="blue") +
  geom_smooth(formula=y~x, method="lm", se=FALSE) +
  stat_regline_equation(label.x=250, label.y=30)
```

For $n$ observations $(x^{(i)}, y^{(i)})$, the fit is found by minimizing the residual sum of squares:

\begin{equation}
  \min_{\beta_0,\beta_1} \left( \sum_{i=1}^n {\left({ y^{(i)} - \beta_0 - \beta_{1}x^{(i)} }\right)}^2 \right).
\end{equation}

---

## Multiple Linear Regression

Typically in real data, we have **multiple** predictors for a response variable.

Given $n$ observations of

- response variable $y$,
- vector of $p$ predictors, ${\bf x} = (x_{1}, x_{2}, \ldots, x_{p})$,

the linear regression model is

$$
 y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_px_p + \varepsilon,
$$

again, fit using least squares procedure:

\begin{equation}
  \min_{\beta_0, \beta_1, \ldots, \beta_p} \left(\sum_{i=1}^n {\left( y^{(i)} - \beta_0 - \sum_{i=1}^p\beta_{j}x_{j}^{(i)} \right)}^2 \right) =: \min_{{\bf\beta}}(\operatorname{RSS}).
\end{equation}

---
class: inverse, middle, center

## What are the practical issues here?

$$
 y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \cdots + \beta_px_p
$$

---

```{r prepare-data, echo=FALSE}
loans_ids <- loans_full_schema |>
  mutate(
    credit_util = total_credit_utilized / total_credit_limit,
    bankruptcy = as.factor(if_else(public_record_bankrupt == 0, 0, 1)),
    income_ = droplevels(verified_income)
  ) |>
  rename(
    credit_checks = inquiries_last_12m,
    in_employment = emp_length 
  ) |> select(
  interest_rate,
  income_,
  debt_to_income,
  credit_util,
  bankruptcy,
  term,
  credit_checks,
  issue_month
)
```

## Example Linear Model

In <abbr title="Introduction to Data Science, one of the first year maths courses here in Edinburgh.">IDS</abbr>, we fit a linear model to the [Lending Club's `loans` dataset](https://openintrostat.github.io/openintro/reference/loans_full_schema.html).

```{r analyse_ids, echo=FALSE}
model_ids <- lm(interest_rate~., data=loans_ids)

model_ids |>
  tidy() |>
  mutate(p.value = ifelse(p.value < 0.001, "< 0.001", round(p.value, 3))) |>
  kbl(linesep="", booktabs=TRUE, digits=2, align="lrrrr") |>
  kable_styling(
    bootstrap_options = c("striped", "condensed"),
    latex_options = c("striped")
  ) |>
  column_spec(1, width="15em", monospace=TRUE) |>
  column_spec(2:5, width="5em")
```

---
class: inverse, middle, center

# 2. Subset Selection

---

## Model Selection

We have a model that works with all the predictors, but could we do better?

> _"Of two competing theories, the simpler explanation is to be preferred."_ (Ockham's Razor)

--

Recall, to compare models of differing size we can use a metric such as the **Akaike information criterion**, which for least squares is

\begin{equation}
  \operatorname{AIC} = \frac{1}{n}\left( {\operatorname{RSS} + 2d\hat{\sigma}^2} \right).
\end{equation}

or the adjusted $R^2$ value,

\begin{equation}
  \operatorname{Adjusted}~R^2 = 1 - \frac{\operatorname{RSS}/(n-d-1)}{\operatorname{TSS}/(n-1)},
\end{equation}

where $\operatorname{TSS} = \sum_i {(y_i-\bar{y})}^2$ and $d$ is the number of variables in the model.

---

### Backward Selection

Starting with full model, successively remove predictors until the criterion can't improve.

```{r backwards, echo=TRUE, class.output="scroll-300"}
full_model <- lm(interest_rate~., data=loans_ids)
backward <- step(full_model, direction='backward')
```

---

### Forward Selection

Starting with the empty model, successively add predictors until criterion can't improve.

```{r forwards, echo=TRUE, class.output="scroll-300"}
empty_model <- lm(interest_rate~1, data=na.omit(loans_ids))
forward <- step(empty_model, direction='forward', scope = formula(full_model))
```
---

### Hybrid Approaches

Backward selection requires that $n > p$ in order to yield a unique least-squares solution. Forward selection works with high-dimensional data when $n < p$ (potentially $n \ll p$).

--

Both beat exhaustion, going from checking $2^p$ models ([NP-complete](https://en.wikipedia.org/wiki/NP-completeness)) to $1+\frac{p(p+1)}{2}$ ([P](https://en.wikipedia.org/wiki/P_(complexity)), but neither are guaranteed to find the optimal $d$-predictor model.

--

In practice, we often take combined approach: removing obsolete variables from the model whenever a new one is added.

```r
empty_model <- lm(interest_rate~., data=loans_ids)
hybrid <- step(empty_model, direction='both')
```

The aim is to mimic the 'benefits' of an exhaustive search while retaining the efficiency of subset selection.

--

Both work well if $p \ll d$, but **neither is performant when we expect the model to use even a moderate number of variables.**

---
class: inverse, middle, center

# 3. Shrinkage Methods

---

## The Lasso

Subset selection aren't the only way to tune a model. Here's a more _optimum_ approach.

More text should follow here. 

---

### Tuning

Something about parameter tuning.

---

## The Ridge

Hello, world!

---

## Least Angle Regression

Hello, world!

---
class: inverse, middle, center

# 4. Dimension Reduction

---

## Placeholder

Hello, world!

---
class: inverse, middle, center

# End
